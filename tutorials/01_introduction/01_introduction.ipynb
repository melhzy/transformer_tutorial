{\n "cells": [\n  {\n   "cell_type": "markdown",\n   "metadata": {},\n   "source": [\n    "# Tutorial 01: Introduction and Setup\n",\n    "\n",\n    "Welcome to the Transformer Tutorial Series! This tutorial covers:\n",\n    "- Environment setup\n",\n    "- Core dependencies\n",\n    "- Mathematical foundations\n",\n    "\n",\n    "## Overview\n",\n    "Transformers revolutionized natural language processing by introducing the attention mechanism as the primary building block, eliminating the need for recurrence."\n   ]\n  },\n  {\n   "cell_type": "markdown",\n   "metadata": {},\n   "source": [\n    "## 1. Environment Setup"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "# Install required packages\n",\n    "# !pip install torch numpy matplotlib"\n   ]\n  },\n  {\n   "cell_type": "markdown",\n   "metadata": {},\n   "source": [\n    "## 2. Import Core Dependencies"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "import torch\n",\n    "import torch.nn as nn\n",\n    "import torch.nn.functional as F\n",\n    "import numpy as np\n",\n    "import matplotlib.pyplot as plt\n",\n    "import math\n",\n    "\n",\n    "# Set random seed for reproducibility\n",\n    "torch.manual_seed(42)\n",\n    "np.random.seed(42)\n",\n    "\n",\n    "# Check PyTorch version and device\n",\n    "print(f\\"PyTorch version: {torch.__version__}\\")\n",\n    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",\n    "print(f\\"Using device: {device}\\")"\n   ]\n  },\n  {\n   "cell_type": "markdown",\n   "metadata": {},\n   "source": [\n    "## 3. Mathematical Foundations\n",\n    "\n",\n    "### 3.1 Key Concepts\n",\n    "\n",\n    "**Attention Mechanism**: \n",\n    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\left(\\frac{QK^T}{\\sqrt{d_k}}\right)V$$\n",\n    "\n",\n    "**Layer Normalization**:\n",\n    "$$\\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",\n    "\n",\n    "**Positional Encoding**:\n",\n    "$$PE_{(pos, 2i)} = \\sin\left(\\frac{pos}{10000^{2i/d_{model}}}\right)$$\n",\n    "$$PE_{(pos, 2i+1)} = \\cos\left(\\frac{pos}{10000^{2i/d_{model}}}\right)$$"\n   ]\n  },\n  {\n   "cell_type": "markdown",\n   "metadata": {},\n   "source": [\n    "### 3.2 Basic Tensor Operations"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "# Example: Matrix multiplication\n",\n    "A = torch.randn(3, 4)\n",\n    "B = torch.randn(4, 5)\n",\n    "C = torch.matmul(A, B)\n",\n    "\n",\n    "print(f\\"Shape of A: {A.shape}\\")\n",\n    "print(f\\"Shape of B: {B.shape}\\")\n",\n    "print(f\\"Shape of C (A @ B): {C.shape}\\")"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "# Example: Softmax operation\n",\n    "x = torch.randn(2, 3)\n",\n    "print(\"Input tensor:\")\n",\n    "print(x)\n",\n    "print(\"\nSoftmax along last dimension:\")\n",\n    "print(F.softmax(x, dim=-1))\n",\n    "print(\"\nSum of softmax (should be 1.0 for each row):\")\n",\n    "print(F.softmax(x, dim=-1).sum(dim=-1))"\n   ]\n  },\n  {\n   "cell_type": "markdown",\n   "metadata": {},\n   "source": [\n    "## 4. Transformer Architecture Overview"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": [\n    "# Model hyperparameters that we'll use throughout the tutorials\n",\n    "d_model = 512        # Embedding dimension\n",\n    "num_heads = 8        # Number of attention heads\n",\n    "d_ff = 2048          # Feed-forward dimension\n",\n    "num_layers = 6       # Number of encoder/decoder layers\n",\n    "dropout = 0.1        # Dropout rate\n",\n    "max_seq_length = 100 # Maximum sequence length\n",\n    "\n",\n    "print(\"Transformer Hyperparameters:\")\n",\n    "print(f\\"Model dimension (d_model): {d_model}\\")\n",\n    "print(f\\"Number of attention heads: {num_heads}\\")\n",\n    "print(f\\"Feed-forward dimension: {d_ff}\\")\n",\n    "print(f\\"Number of layers: {num_layers}\\")\n",\n    "print(f\\"Dropout rate: {dropout}\\")\n",\n    "print(f\\"Maximum sequence length: {max_seq_length}\\")"\n   ]\n  },\n  {\n   "cell_type": "markdown",\n   "metadata": {},\n   "source": [\n    "## 5. Next Steps\n",\n    "\n",\n    "In the following tutorials, we'll build each component:\n",\n    "1. âœ… **Tutorial 01**: Introduction and Setup (current)\n",\n    "2. **Tutorial 02**: Attention Mechanism\n",\n    "3. **Tutorial 03**: Multi-Head Attention\n",\n    "4. **Tutorial 04**: Positional Encoding\n",\n    "5. **Tutorial 05**: Feed-Forward Networks\n",\n    "6. **Tutorial 06**: Encoder Layer\n",\n    "7. **Tutorial 07**: Decoder Layer\n",\n    "8. **Tutorial 08**: Complete Transformer\n",\n    "\n",\n    "Let's proceed to Tutorial 02 to learn about the attention mechanism!"\n   ]\n  },\n  {\n   "cell_type": "code",\n   "execution_count": null,\n   "metadata": {},\n   "outputs": [],\n   "source": []\n  }\n ],\n "metadata": {\n  "kernelspec": {\n   "display_name": "Python 3",\n   "language": "python",\n   "name": "python3"\n  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "mimetype": "text/x-python",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.8.0"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}