{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 01: Introduction and Setup\n",
    "\n",
    "Welcome to the Transformer Tutorial series! This set of notebooks will guide you through the foundations, implementation, and applications of transformer-based models in deep learning. By the end of this series, you will understand how transformers work from the ground up and be able to implement them yourself.\n",
    "\n",
    "## What Are Transformers?\n",
    "\n",
    "The Transformer is a neural network architecture introduced in the paper **\"Attention Is All You Need\"** (Vaswani et al., 2017). It revolutionized Natural Language Processing and is increasingly used in computer vision, speech recognition, and other domains.\n",
    "\n",
    "### Key Innovations:\n",
    "\n",
    "1. **Eliminating recurrence**: Unlike RNNs and LSTMs, Transformers process entire sequences in parallel\n",
    "2. **Using self-attention**: Allowing each position to attend to all positions in the previous layer\n",
    "3. **Scaling efficiently**: Training much faster on modern hardware (GPUs/TPUs)\n",
    "\n",
    "### Core Attention Mechanism:\n",
    "\n",
    "The fundamental attention operation is:\n",
    "\n",
    "```\n",
    "Attention(Q, K, V) = softmax(QK^T / √d_k)V\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **Q** (Query): What we're looking for\n",
    "- **K** (Key): What we're comparing against\n",
    "- **V** (Value): The actual information we want to retrieve\n",
    "- **d_k**: Dimension of the key vectors (used for scaling)\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn in This Tutorial:\n",
    "- Prerequisites and foundational concepts\n",
    "- Hardware/software environment setup\n",
    "- Installing core dependencies\n",
    "- Verifying your installation\n",
    "- Basic tensor operations for transformers\n",
    "- Project structure overview\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prerequisites\n",
    "\n",
    "Before starting, ensure you have a solid foundation in:\n",
    "\n",
    "### Python Programming\n",
    "- Object-oriented programming\n",
    "- NumPy for array operations\n",
    "- Basic PyTorch usage\n",
    "\n",
    "### Mathematics\n",
    "- **Linear algebra**: Matrix multiplication, vectors, dot products\n",
    "- **Calculus basics**: Gradients, derivatives\n",
    "- **Probability**: Softmax, probability distributions\n",
    "\n",
    "### Deep Learning Concepts\n",
    "- Neural networks and backpropagation\n",
    "- Gradient descent optimization\n",
    "- Embeddings and word representations\n",
    "\n",
    "### System Requirements:\n",
    "- **Python 3.8+** (Recommended: 3.10)\n",
    "- GPU (NVIDIA CUDA) support recommended for faster training (optional)\n",
    "- Familiarity with Jupyter notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup\n",
    "\n",
    "We recommend setting up a virtual environment to avoid dependency conflicts. You can use **virtualenv** or **conda** for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Using virtualenv (Linux/Mac/Windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install virtualenv if not already installed\n",
    "# !pip install virtualenv\n",
    "\n",
    "# Create virtual environment\n",
    "# !python -m venv transformer_env\n",
    "\n",
    "# Activate it:\n",
    "# Linux/Mac: source transformer_env/bin/activate\n",
    "# Windows: transformer_env\\Scripts\\activate\n",
    "\n",
    "print(\"Virtual environment setup commands (uncomment to run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Using conda (Anaconda/Miniconda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create conda environment\n",
    "# !conda create -n transformer-env python=3.10 -y\n",
    "\n",
    "# Activate it:\n",
    "# !conda activate transformer-env\n",
    "\n",
    "print(\"Conda environment setup commands (uncomment to run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Required Libraries\n",
    "\n",
    "We'll use PyTorch as our main framework, along with NumPy, Matplotlib, and other utilities.\n",
    "\n",
    "**Note**: If you're using CUDA (GPU acceleration), refer to [PyTorch's installation guide](https://pytorch.org/get-started/locally/) for the correct commands for your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch (CPU version)\n",
    "# For GPU version, check: https://pytorch.org/get-started/locally/\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "# Install other dependencies\n",
    "# !pip install numpy matplotlib tqdm\n",
    "\n",
    "# Optional: For working with pre-trained models\n",
    "# !pip install transformers datasets\n",
    "\n",
    "print(\"Installation commands (uncomment to run if not already installed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Environment Verification\n",
    "\n",
    "Let's verify that all required libraries are installed correctly and check your system configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Check Python Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def check_python_version():\n",
    "    \"\"\"Check if Python version is 3.8 or higher.\"\"\"\n",
    "    version = sys.version_info\n",
    "    print(f\"Python Version: {version.major}.{version.minor}.{version.micro}\")\n",
    "    if version.major >= 3 and version.minor >= 8:\n",
    "        print(\"✓ Python version is compatible\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"✗ Python version must be 3.8 or higher\")\n",
    "        return False\n",
    "\n",
    "check_python_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Check PyTorch Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_torch():\n",
    "    \"\"\"Check if PyTorch is installed and working.\"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        print(f\"✓ PyTorch version: {torch.__version__}\")\n",
    "        \n",
    "        # Test basic tensor operations\n",
    "        x = torch.tensor([1.0, 2.0, 3.0])\n",
    "        y = torch.tensor([4.0, 5.0, 6.0])\n",
    "        z = x + y\n",
    "        print(f\"  Basic operation test: {x.tolist()} + {y.tolist()} = {z.tolist()}\")\n",
    "        \n",
    "        # Check CUDA availability\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"  ✓ CUDA available: Yes (Device: {torch.cuda.get_device_name(0)})\")\n",
    "        else:\n",
    "            print(f\"  ℹ CUDA available: No (using CPU - this is fine for learning!)\")\n",
    "        \n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(\"✗ PyTorch is not installed\")\n",
    "        print(\"  Install it with: pip install torch\")\n",
    "        return False\n",
    "\n",
    "check_torch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Check NumPy Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_numpy():\n",
    "    \"\"\"Check if NumPy is installed and working.\"\"\"\n",
    "    try:\n",
    "        import numpy as np\n",
    "        print(f\"✓ NumPy version: {np.__version__}\")\n",
    "        \n",
    "        # Test basic array operations\n",
    "        a = np.array([1, 2, 3])\n",
    "        b = np.array([4, 5, 6])\n",
    "        c = a + b\n",
    "        print(f\"  Basic operation test: {a.tolist()} + {b.tolist()} = {c.tolist()}\")\n",
    "        \n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(\"✗ NumPy is not installed\")\n",
    "        print(\"  Install it with: pip install numpy\")\n",
    "        return False\n",
    "\n",
    "check_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic Concepts Demonstration\n",
    "\n",
    "Let's explore the fundamental operations you'll encounter throughout these tutorials. Understanding these concepts is crucial for implementing transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Tensor Shapes (Critical for Understanding Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Common transformer dimensions\n",
    "batch_size = 2      # Number of sequences processed together\n",
    "seq_length = 4      # Number of tokens in each sequence\n",
    "d_model = 8         # Embedding dimension (size of each token representation)\n",
    "\n",
    "# Simulating a batch of sequences\n",
    "x = torch.randn(batch_size, seq_length, d_model)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TENSOR SHAPES\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"  - Dimension 0 (batch_size={batch_size}): Number of sequences\")\n",
    "print(f\"  - Dimension 1 (seq_length={seq_length}): Number of tokens per sequence\")\n",
    "print(f\"  - Dimension 2 (d_model={d_model}): Embedding dimension per token\")\n",
    "print(f\"\\nThis represents {batch_size} sequences, each with {seq_length} tokens,\")\n",
    "print(f\"where each token is represented as a {d_model}-dimensional vector.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Matrix Multiplication (Core of Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query and Key matrices for attention\n",
    "Q = torch.randn(seq_length, d_model)\n",
    "K = torch.randn(seq_length, d_model)\n",
    "\n",
    "# Attention scores: Q @ K^T\n",
    "# This creates a matrix showing how much each token attends to every other token\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MATRIX MULTIPLICATION IN ATTENTION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Q (Query) shape: {Q.shape}\")\n",
    "print(f\"K (Key) shape: {K.shape}\")\n",
    "print(f\"K^T (Key transposed) shape: {K.transpose(-2, -1).shape}\")\n",
    "print(f\"\\nAttention scores (Q @ K^T) shape: {scores.shape}\")\n",
    "print(f\"\\nThis creates a {seq_length}×{seq_length} attention matrix where:\")\n",
    "print(f\"  - Each row represents one token's attention scores\")\n",
    "print(f\"  - Each column represents how much that position is attended to\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Softmax (Turning Scores into Probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example attention scores\n",
    "sample_scores = torch.tensor([\n",
    "    [1.0, 2.0, 3.0],\n",
    "    [4.0, 5.0, 6.0]\n",
    "])\n",
    "\n",
    "# Apply softmax to convert scores to probabilities\n",
    "probabilities = torch.softmax(sample_scores, dim=-1)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SOFTMAX OPERATION\")\n",
    "print(\"=\"*60)\n",
    "print(\"Input scores (raw attention scores):\")\n",
    "print(sample_scores)\n",
    "print(\"\\nAfter softmax (attention probabilities):\")\n",
    "print(probabilities)\n",
    "print(f\"\\nSum of each row: {probabilities.sum(dim=-1)}\")\n",
    "print(\"Note: Each row sums to 1.0 (valid probability distribution)\")\n",
    "print(\"\\nHigher input scores become higher probabilities.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Broadcasting (Used in Positional Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating adding positional encoding to embeddings\n",
    "embeddings = torch.ones(3, 4)  # 3 tokens, 4-dimensional embeddings\n",
    "position_bias = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "\n",
    "# Broadcasting automatically expands position_bias to match embeddings shape\n",
    "result = embeddings + position_bias\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BROADCASTING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Position bias shape: {position_bias.shape}\")\n",
    "print(f\"Result shape: {result.shape}\")\n",
    "print(\"\\nOriginal embeddings:\")\n",
    "print(embeddings)\n",
    "print(\"\\nPosition bias:\")\n",
    "print(position_bias)\n",
    "print(\"\\nResult (embeddings + position_bias):\")\n",
    "print(result)\n",
    "print(\"\\nThe bias is automatically broadcast to match embedding dimensions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Complete Mini-Example: Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Simplified implementation of scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Query matrix (seq_len, d_k)\n",
    "        K: Key matrix (seq_len, d_k)\n",
    "        V: Value matrix (seq_len, d_v)\n",
    "    \n",
    "    Returns:\n",
    "        Attention output (seq_len, d_v)\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "    \n",
    "    # 1. Compute attention scores: Q @ K^T\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "    \n",
    "    # 2. Scale by sqrt(d_k) to prevent vanishing gradients\n",
    "    scores = scores / math.sqrt(d_k)\n",
    "    \n",
    "    # 3. Apply softmax to get attention weights\n",
    "    attention_weights = torch.softmax(scores, dim=-1)\n",
    "    \n",
    "    # 4. Apply weights to values: attention_weights @ V\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Example usage\n",
    "seq_len = 3\n",
    "d_k = 4\n",
    "d_v = 4\n",
    "\n",
    "Q = torch.randn(seq_len, d_k)\n",
    "K = torch.randn(seq_len, d_k)\n",
    "V = torch.randn(seq_len, d_v)\n",
    "\n",
    "output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SCALED DOT-PRODUCT ATTENTION EXAMPLE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Input shapes:\")\n",
    "print(f\"  Q: {Q.shape}\")\n",
    "print(f\"  K: {K.shape}\")\n",
    "print(f\"  V: {V.shape}\")\n",
    "print(f\"\\nAttention weights shape: {attention_weights.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nAttention weights (how much each token attends to others):\")\n",
    "print(attention_weights)\n",
    "print(f\"\\nEach row sums to 1: {attention_weights.sum(dim=-1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Project Structure\n",
    "\n",
    "Here's how this repository is organized:\n",
    "\n",
    "```\n",
    "transformer_tutorial/\n",
    "├── README.md                    # Main documentation\n",
    "├── requirements.txt             # Python dependencies\n",
    "├── tutorials/\n",
    "│   ├── 01_introduction/         # This tutorial\n",
    "│   │   ├── 01_introduction.ipynb\n",
    "│   │   ├── README.md\n",
    "│   │   └── example.py\n",
    "│   ├── 02_attention_mechanism/  # Deep dive into attention\n",
    "│   ├── 03_multi_head_attention/ # Multi-head attention\n",
    "│   ├── 04_positional_encoding/  # Adding position information\n",
    "│   ├── 05_feed_forward/         # Feed-forward networks\n",
    "│   ├── 06_encoder_layer/        # Complete encoder layer\n",
    "│   ├── 07_decoder_layer/        # Complete decoder layer\n",
    "│   └── 08_complete_transformer/ # Full transformer model\n",
    "```\n",
    "\n",
    "Each tutorial builds on the previous ones:\n",
    "1. **Introduction** (this notebook): Setup and basic concepts\n",
    "2. **Attention Mechanism**: Understanding and implementing attention\n",
    "3. **Multi-Head Attention**: Parallel attention mechanisms\n",
    "4. **Positional Encoding**: Adding sequence order information\n",
    "5. **Feed-Forward Networks**: Position-wise transformations\n",
    "6. **Encoder Layer**: Combining components into encoder\n",
    "7. **Decoder Layer**: Building the decoder with masked attention\n",
    "8. **Complete Transformer**: Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Transformer Architecture Overview\n",
    "\n",
    "Before we dive deep into the details, here's a high-level view of the complete architecture:\n",
    "\n",
    "```\n",
    "Input Sequence (e.g., \"Hello world\")\n",
    "        ↓\n",
    "[Input Embedding + Positional Encoding]\n",
    "        ↓\n",
    "┌─────────────────────────────────────┐\n",
    "│   ENCODER (N=6 identical layers)   │\n",
    "│                                     │\n",
    "│   For each layer:                  │\n",
    "│   ┌──────────────────────────────┐ │\n",
    "│   │ 1. Multi-Head Self-Attention │ │\n",
    "│   │    - Queries, Keys, Values   │ │\n",
    "│   │    - 8 parallel attention    │ │\n",
    "│   └──────────────────────────────┘ │\n",
    "│            ↓                        │\n",
    "│   [Add & Normalize]                │\n",
    "│            ↓                        │\n",
    "│   ┌──────────────────────────────┐ │\n",
    "│   │ 2. Feed-Forward Network      │ │\n",
    "│   │    - Two linear layers       │ │\n",
    "│   │    - ReLU activation         │ │\n",
    "│   └──────────────────────────────┘ │\n",
    "│            ↓                        │\n",
    "│   [Add & Normalize]                │\n",
    "└─────────────────────────────────────┘\n",
    "        ↓\n",
    "[Encoder Output]\n",
    "        ↓\n",
    "┌─────────────────────────────────────┐\n",
    "│   DECODER (N=6 identical layers)   │\n",
    "│                                     │\n",
    "│   For each layer:                  │\n",
    "│   ┌──────────────────────────────┐ │\n",
    "│   │ 1. Masked Self-Attention     │ │\n",
    "│   │    - Prevents future peeking │ │\n",
    "│   └──────────────────────────────┘ │\n",
    "│            ↓                        │\n",
    "│   [Add & Normalize]                │\n",
    "│            ↓                        │\n",
    "│   ┌──────────────────────────────┐ │\n",
    "│   │ 2. Cross-Attention           │ │\n",
    "│   │    - Attends to encoder      │ │\n",
    "│   └──────────────────────────────┘ │\n",
    "│            ↓                        │\n",
    "│   [Add & Normalize]                │\n",
    "│            ↓                        │\n",
    "│   ┌──────────────────────────────┐ │\n",
    "│   │ 3. Feed-Forward Network      │ │\n",
    "│   └──────────────────────────────┘ │\n",
    "│            ↓                        │\n",
    "│   [Add & Normalize]                │\n",
    "└─────────────────────────────────────┘\n",
    "        ↓\n",
    "[Linear Layer + Softmax]\n",
    "        ↓\n",
    "Output Probabilities\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Common Applications\n",
    "\n",
    "Transformers are used in numerous real-world applications:\n",
    "\n",
    "### Natural Language Processing\n",
    "- **Machine Translation**: Translating text between languages (e.g., Google Translate)\n",
    "- **Text Summarization**: Condensing long documents into summaries\n",
    "- **Question Answering**: Finding answers in text (e.g., search engines)\n",
    "- **Text Generation**: Creating coherent text (e.g., GPT models, ChatGPT)\n",
    "- **Sentiment Analysis**: Understanding emotions in text\n",
    "- **Named Entity Recognition**: Identifying people, places, organizations\n",
    "\n",
    "### Computer Vision\n",
    "- **Image Classification**: Vision Transformers (ViT)\n",
    "- **Object Detection**: DETR (Detection Transformer)\n",
    "- **Image Generation**: DALL-E, Stable Diffusion\n",
    "\n",
    "### Multimodal Applications\n",
    "- **Image Captioning**: Describing images with text\n",
    "- **Visual Question Answering**: Answering questions about images\n",
    "- **Text-to-Image Generation**: Creating images from descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Running the Notebooks\n",
    "\n",
    "You can run these notebooks in several ways:\n",
    "\n",
    "### Local Installation\n",
    "```bash\n",
    "# Install Jupyter\n",
    "pip install notebook\n",
    "jupyter notebook\n",
    "\n",
    "# Or Jupyter Lab\n",
    "pip install jupyterlab\n",
    "jupyter lab\n",
    "```\n",
    "\n",
    "### Cloud Platforms\n",
    "- **Google Colab**: Free GPU access - [colab.research.google.com](https://colab.research.google.com/)\n",
    "- **Kaggle Kernels**: Free GPU/TPU access - [kaggle.com/kernels](https://www.kaggle.com/kernels)\n",
    "- **AWS SageMaker**: Professional cloud environment\n",
    "\n",
    "### Tips for Cloud Platforms\n",
    "- Upload notebooks directly or clone the repository\n",
    "- Make sure to enable GPU runtime for faster execution\n",
    "- Save your work frequently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Tips for Success\n",
    "\n",
    "### Learning Strategies\n",
    "1. **Read and experiment**: Don't just run the cells—modify them and see what happens\n",
    "2. **Understand the math**: Work through the mathematical operations step by step\n",
    "3. **Visualize**: Draw diagrams of tensor shapes and data flow\n",
    "4. **Code from scratch**: Try implementing components yourself before looking at solutions\n",
    "5. **Ask questions**: Use GitHub Issues or Discussions for help\n",
    "\n",
    "### Debugging Tips\n",
    "- **Print tensor shapes**: Use `.shape` frequently to understand dimensions\n",
    "- **Use small examples**: Test with tiny tensors first (e.g., seq_len=3, d_model=4)\n",
    "- **Check intermediate results**: Print outputs at each step\n",
    "- **Read error messages**: PyTorch errors often indicate dimension mismatches\n",
    "\n",
    "### Resources\n",
    "- [Original Paper: \"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762)\n",
    "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - Excellent visual guide\n",
    "- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)\n",
    "- [Hugging Face Transformers](https://huggingface.co/docs/transformers/) - Pre-trained models\n",
    "- [Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/) - Line-by-line implementation\n",
    "\n",
    "### Version Control\n",
    "Consider using git to track your progress:\n",
    "```bash\n",
    "git init\n",
    "git add .\n",
    "git commit -m \"Completed introduction tutorial\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "In this tutorial, you:\n",
    "\n",
    "✅ Learned what Transformers are and why they're important  \n",
    "✅ Set up your development environment  \n",
    "✅ Verified your installation  \n",
    "✅ Explored fundamental tensor operations  \n",
    "✅ Implemented a simple attention mechanism  \n",
    "✅ Understood the project structure  \n",
    "✅ Got an overview of the complete architecture  \n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Tutorial 02: Attention Mechanism**, you'll:\n",
    "- Understand the mathematics behind attention\n",
    "- Implement scaled dot-product attention from scratch\n",
    "- Visualize attention weights\n",
    "- Explore different attention variants\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to continue?** → [Tutorial 02: Attention Mechanism](../02_attention_mechanism/)\n",
    "\n",
    "---\n",
    "\n",
    "*If you found this helpful, consider ⭐ starring the repository on GitHub!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
